# ğŸ§  DetecÃ§Ã£o de EmoÃ§Ãµes em Tempo Real: YOLO-V5 + RepVGG

## IntroduÃ§Ã£o
Este projeto consiste em um sistema de **DetecÃ§Ã£o e ClassificaÃ§Ã£o de EmoÃ§Ãµes Faciais em Tempo Real** utilizando uma arquitetura robusta baseada em **VisÃ£o Computacional** e **Deep Learning (PyTorch)**.  
O objetivo Ã© identificar faces em um vÃ­deo ou imagem e classificar a emoÃ§Ã£o expressa em uma das **8 categorias bÃ¡sicas**.

## ğŸ—ï¸ Arquitetura do Modelo
O sistema opera em duas etapas sequenciais para garantir alta precisÃ£o e velocidade:

### 1. DetecÃ§Ã£o Facial (YOLO-V5)
Utiliza uma versÃ£o otimizada do **YOLO-V5 (You Only Look Once)**, treinado no massivo dataset **WIDER FACE**, para identificar e gerar caixas delimitadoras (*bounding boxes*) precisas para cada rosto presente na cena.

### 2. ClassificaÃ§Ã£o de EmoÃ§Ãµes (RepVGG)
A regiÃ£o da face detectada Ã© entÃ£o alimentada em uma rede neural **RepVGG**.  
Este modelo, conhecido por sua alta eficiÃªncia e desempenho em tarefas de classificaÃ§Ã£o, foi treinado no dataset **AffectNet** para classificar a emoÃ§Ã£o.

## ğŸ˜ƒ EmoÃ§Ãµes Detectadas
O modelo classifica **8 expressÃµes faciais primÃ¡rias**:

- ğŸ˜¡ anger (Raiva)  
- ğŸ˜’ contempt (Desprezo)  
- ğŸ¤® disgust (Nojo)  
- ğŸ˜¨ fear (Medo)  
- ğŸ˜„ happy (Felicidade)  
- ğŸ˜ neutral (Neutro)  
- ğŸ˜¢ sad (Tristeza)  
- ğŸ˜® surprise (Surpresa)  

## ğŸ› ï¸ ConfiguraÃ§Ã£o do Ambiente
Este projeto requer **Python 3.7+** e as bibliotecas listadas no `requirements.txt`.

### 1. Clonar o RepositÃ³rio
```bash
git clone https://github.com/luizfernandodag/emotion-detection-pytorch.git
cd emotion-detection-pytorch

```

### 2. InstalaÃ§Ã£o de DependÃªncias

#### Usando pip (Recomendado)
```bash
# Crie um ambiente virtual (opcional, mas boa prÃ¡tica)
python -m venv venv
source venv/bin/activate  # Linux/macOS
# venv\Scripts\activate # Windows

# Instale as dependÃªncias
pip install -r requirements.txt
```

#### Usando conda (Alternativa)
```bash
conda env create -f env.yaml
conda activate yolov5
```

### 3. Pesos do Modelo (Weights)
O projeto estÃ¡ configurado para **baixar automaticamente os pesos prÃ©-treinados** na primeira execuÃ§Ã£o.

## â–¶ï¸ Como Rodar o Projeto
O script principal Ã© o `main.py`, que aceita a fonte (`source`) da sua entrada de vÃ­deo/imagem.

### 1. DetecÃ§Ã£o em Tempo Real (Webcam)
```bash
python main.py --source 0
```

### 2. DetecÃ§Ã£o em Arquivo de VÃ­deo ou Imagem
```bash
python main.py --source /caminho/para/seu/video_ou_imagem.mp4
```

## âš™ï¸ OpÃ§Ãµes Adicionais (Argumentos)

| Argumento        | DescriÃ§Ã£o                                              | Exemplo              |
|------------------|----------------------------------------------------------|----------------------|
| `--source`       | Fonte da inferÃªncia (cÃ¢mera, vÃ­deo, imagem)             | 0 ou video.mp4       |
| `--conf-thres`   | Limite de confianÃ§a da detecÃ§Ã£o facial (0.0 a 1.0)      | 0.45                 |
| `--device`       | Dispositivo de hardware para inferÃªncia                 | cpu ou 0 (CUDA)      |
| `--hide-img`     | NÃ£o exibir a janela de resultados                       | â€”                    |
| `--output-path`  | Salvar os resultados em um diretÃ³rio                    | ./saida_analisada    |

## ğŸ“¸ DemonstraÃ§Ã£o da AplicaÃ§Ã£o

![Exemplo de detecÃ§Ã£o de emoÃ§Ãµes](./detector_img.jpeg)


## â˜ï¸ Link para Teste Online (AWS)
A fazer 
ğŸ”— **Web App:**  
```
A fazer 
```

## ğŸ“„ Sobre o Autor
Este projeto foi desenvolvido por **[LUIZ FERNANDO DE ANDRADE GADELHA]** como parte do curso de Engenharia de VisÃ£o Computacional.
- LinkedIn:  luiz-gadelha
- Artigo publicado: A FAZER
